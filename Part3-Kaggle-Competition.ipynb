{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what we have in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset = pd.read_csv('tweet_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print few rows to check the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    People who post \"add me on #Snapchat\" must be ...\n",
       "1    @brianklaas As we see, Trump is dangerous to #...\n",
       "2    Confident of your obedience, I write to you, k...\n",
       "3                  Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>\n",
       "4    \"Trust is not the same as faith. A friend is s...\n",
       "5    @RISKshow @TheKevinAllison Thx for the BEST TI...\n",
       "6         Still waiting on those supplies Liscus. <LH>\n",
       "7                        Love knows no gender. ðŸ˜¢ðŸ˜­ <LH>\n",
       "8    @DStvNgCare @DStvNg More highlights are being ...\n",
       "9    When do you have enough ? When are you satisfi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_dataset[:10]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove some irrelevant data, such as links, numbers, punctuation. Also, I try to add hasgtags data in the text to serve as additional information. Also, coonvert all the text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess_text(df):\n",
    "    text = df['text']\n",
    "    hashtags = df['hashtags']\n",
    "    text = re.sub(r'<LH>', '', text)\n",
    "    t3ext = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    text = text + ' '+' '.join(eval(hashtags))\n",
    "    #lowercase text\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)  # remove punctuation\n",
    "    text = \" \".join(text.split())  # remove extra spaces, tabs, and new lines\n",
    "    return text\n",
    "\n",
    "tweet_dataset['preprocess_text'] = tweet_dataset.apply(preprocess_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text after preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>people who post add me on snapchat must be deh...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>brianklaas as we see trump is dangerous to fre...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>now issa is stalking tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>riskshow thekevinallison thx for the best time...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>still waiting on those supplies liscus</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>im so happy nowonder the name of this show hap...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>in every circumtance id like to be thankful to...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>theres currently two girls walking around the ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>ah corporate life where you can date using jus...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>blessed to be living sundayvibes sundayvibes</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                    preprocess_text  \\\n",
       "0        0x376b20  people who post add me on snapchat must be deh...   \n",
       "1        0x2d5350  brianklaas as we see trump is dangerous to fre...   \n",
       "2        0x1cd5b0                     now issa is stalking tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚   \n",
       "3        0x1d755c  riskshow thekevinallison thx for the best time...   \n",
       "4        0x2c91a8             still waiting on those supplies liscus   \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  im so happy nowonder the name of this show hap...   \n",
       "1455559  0x38959e  in every circumtance id like to be thankful to...   \n",
       "1455560  0x2cbca6  theres currently two girls walking around the ...   \n",
       "1455561  0x24faed  ah corporate life where you can date using jus...   \n",
       "1455562  0x34be8c       blessed to be living sundayvibes sundayvibes   \n",
       "\n",
       "              emotion  \n",
       "0        anticipation  \n",
       "1             sadness  \n",
       "2                fear  \n",
       "3                 joy  \n",
       "4        anticipation  \n",
       "...               ...  \n",
       "1455558           joy  \n",
       "1455559           joy  \n",
       "1455560           joy  \n",
       "1455561           joy  \n",
       "1455562           joy  \n",
       "\n",
       "[1455563 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Number of training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset: 1455563\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of training dataset: {len(train_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the Label distribution\n",
    "We have unbalanced data in the dataset, we may need to deal with it in the future step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "anger            39867\n",
       "anticipation    248935\n",
       "disgust         139101\n",
       "fear             63999\n",
       "joy             516017\n",
       "sadness         193437\n",
       "surprise         48729\n",
       "trust           205478\n",
       "Name: preprocess_text, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the emotion\n",
    "count_df = train_dataset.groupby(['emotion']).count()['preprocess_text']\n",
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# the histogram of the data\n",
    "labels = train_dataset['emotion'].unique()\n",
    "post_total = len(train_dataset)\n",
    "count_df = count_df.apply(lambda x: round(x*100/post_total,3))\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "plt.bar(count_df.index, count_df.values)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Using decision tree and bag of word as first apporach. Set max feature as 500 to avoid the feature size to be too large. We set max depth=20, because if we don't specify it, the running time will be too long. The final accuracy in testing set is roughly 42-43%. As what I experiment, setting depth to 20 is better than 40. I guess if the tree is too deep, it may lead to overfitting. The performance will be worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df= train_test_split(train_dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "# Using bag of words and set the max feature=500 to avoid the feature be too large\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "BOW_500.fit(train_df['preprocess_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = BOW_500.transform(train_df['preprocess_text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_df['preprocess_text'])\n",
    "y_test = test_df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Result of using Decision Tree model --\n",
      "Training accuracy: 0.42\n",
      "Testing accuracy: 0.42\n",
      "\n",
      " -- Classification report of using testing dataset --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.05      0.00      0.00      7946\n",
      "anticipation       0.75      0.23      0.35     49984\n",
      "     disgust       0.76      0.05      0.09     27669\n",
      "        fear       0.34      0.00      0.00     12846\n",
      "         joy       0.38      0.97      0.55    102943\n",
      "     sadness       0.56      0.20      0.29     38745\n",
      "    surprise       0.42      0.00      0.00      9816\n",
      "       trust       0.70      0.03      0.05     41164\n",
      "\n",
      "    accuracy                           0.42    291113\n",
      "   macro avg       0.50      0.18      0.17    291113\n",
      "weighted avg       0.54      0.42      0.31    291113\n",
      "\n",
      "\n",
      " -- Confusion matrix of using testing dataset --\n",
      "[[    1    49    51     0  7329   512     0     4]\n",
      " [    3 11431    30     2 37640   793     2    83]\n",
      " [    5   183  1362     1 23984  2128     0     6]\n",
      " [    1    96    47    13 12229   456     0     4]\n",
      " [    3  1983    74    16 99438  1051     9   369]\n",
      " [    6   224   174     3 30602  7710     3    23]\n",
      " [    1    65    21     1  9181   533    10     4]\n",
      " [    1  1298    23     2 38172   505     0  1163]]\n"
     ]
    }
   ],
   "source": [
    "# build DecisionTree model and set max depth to 20\n",
    "DT_model_1 = DecisionTreeClassifier(random_state=1, max_depth=20)\n",
    "\n",
    "# training\n",
    "DT_model_1 = DT_model_1.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_train_pred = DT_model_1.predict(X_train)\n",
    "y_test_pred = DT_model_1.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('\\n -- Result of using Decision Tree model --')\n",
    "print('Training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('Testing accuracy: {}'.format(round(acc_test, 2)))\n",
    "\n",
    "# precision, recall, f1-score\n",
    "print('\\n -- Classification report of using testing dataset --')\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))\n",
    "\n",
    "# check by confusion matrix\n",
    "print('\\n -- Confusion matrix of using testing dataset --')\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Result of using Decision Tree(max depth=40) model --\n",
      "Training accuracy: 0.47\n",
      "Testing accuracy: 0.43\n",
      "\n",
      " -- Classification report of using testing dataset --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.12      0.00      0.01      7946\n",
      "anticipation       0.68      0.29      0.40     49984\n",
      "     disgust       0.54      0.08      0.13     27669\n",
      "        fear       0.40      0.03      0.05     12846\n",
      "         joy       0.40      0.94      0.56    102943\n",
      "     sadness       0.54      0.21      0.30     38745\n",
      "    surprise       0.24      0.00      0.01      9816\n",
      "       trust       0.60      0.11      0.18     41164\n",
      "\n",
      "    accuracy                           0.43    291113\n",
      "   macro avg       0.44      0.21      0.20    291113\n",
      "weighted avg       0.49      0.43      0.35    291113\n",
      "\n",
      "\n",
      " -- Confusion matrix of using testing dataset --\n",
      "[[   23   168   175    11  6986   521     2    60]\n",
      " [   24 14274   236   143 33675   944    31   657]\n",
      " [   28   515  2114    42 22428  2380    18   144]\n",
      " [   10   288   125   340 11537   463     2    81]\n",
      " [   41  3005   334   167 96319  1316    52  1709]\n",
      " [   33   698   689    57 28881  8123    33   231]\n",
      " [    8   169   112    11  8842   566    49    59]\n",
      " [   18  1818   143    72 34103   660    20  4330]]\n"
     ]
    }
   ],
   "source": [
    "# build DecisionTree model and set max depth = 40\n",
    "DT_model_1 = DecisionTreeClassifier(random_state=1, max_depth=40)\n",
    "\n",
    "# training\n",
    "DT_model_1 = DT_model_1.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_train_pred = DT_model_1.predict(X_train)\n",
    "y_test_pred = DT_model_1.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('\\n -- Result of using Decision Tree(max depth=40) model --')\n",
    "print('Training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('Testing accuracy: {}'.format(round(acc_test, 2)))\n",
    "\n",
    "# precision, recall, f1-score\n",
    "print('\\n -- Classification report of using testing dataset --')\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))\n",
    "\n",
    "# check by confusion matrix\n",
    "print('\\n -- Confusion matrix of using testing dataset --')\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second approach I try is using the naive model. The testing result is not really different from using the decision model. It's approximately 42%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Result of using Naive Bayes model --\n",
      "Training accuracy: 0.42\n",
      "Testing accuracy: 0.42\n",
      "\n",
      "-- Classification report of testing dataset --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.12      0.07      0.09      7946\n",
      "anticipation       0.46      0.43      0.45     49984\n",
      "     disgust       0.29      0.28      0.28     27669\n",
      "        fear       0.21      0.13      0.16     12846\n",
      "         joy       0.47      0.67      0.55    102943\n",
      "     sadness       0.38      0.32      0.35     38745\n",
      "    surprise       0.25      0.07      0.11      9816\n",
      "       trust       0.36      0.19      0.25     41164\n",
      "\n",
      "    accuracy                           0.42    291113\n",
      "   macro avg       0.32      0.27      0.28    291113\n",
      "weighted avg       0.39      0.42      0.39    291113\n",
      "\n",
      "\n",
      " -- Confusion Matrix of testing dataset --\n",
      "[[  546   694  1288   231  3567  1235    79   306]\n",
      " [  478 21738  2824  1031 17578  3277   224  2834]\n",
      " [  689  2469  7758   894  9993  4731   309   826]\n",
      " [  195  1201  1427  1719  6515  1291   100   398]\n",
      " [ 1061 11789  4939  2229 68593  5856   655  7821]\n",
      " [  886  3226  5509  1038 13888 12511   403  1284]\n",
      " [  159   825  1358   362  4478  1591   663   380]\n",
      " [  408  5759  2086   746 21479  2541   219  7926]]\n"
     ]
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('-- Result of using Naive Bayes model --')\n",
    "print('Training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('Testing accuracy: {}'.format(round(acc_test, 2)))\n",
    "print('\\n-- Classification report of testing dataset --')\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))\n",
    "\n",
    "print('\\n -- Confusion Matrix of testing dataset --')\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model\n",
    "Using deep learning model the testing result is 47%. Also, too much epochs of training will lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 834097        joy\n",
      "355739    sadness\n",
      "625638        joy\n",
      "678647        joy\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_test.shape:  (291113,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1164450, 8)\n",
      "y_test.shape:  (291113, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train.iloc[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                32064     \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " softmax_1 (Softmax)         (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,744\n",
      "Trainable params: 36,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Data input output\n",
    "input_shape = X_train.shape[1]\n",
    "output_shape = len(label_encoder.classes_)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, )) # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X) # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1) # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2) # 8\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36390/36390 [==============================] - 118s 3ms/step - loss: 1.4730 - accuracy: 0.4624 - val_loss: 1.4544 - val_accuracy: 0.4667\n",
      "Epoch 2/5\n",
      "36390/36390 [==============================] - 138s 4ms/step - loss: 1.4379 - accuracy: 0.4752 - val_loss: 1.4426 - val_accuracy: 0.4724\n",
      "Epoch 3/5\n",
      "36390/36390 [==============================] - 203s 6ms/step - loss: 1.4273 - accuracy: 0.4792 - val_loss: 1.4375 - val_accuracy: 0.4746\n",
      "Epoch 4/5\n",
      "36390/36390 [==============================] - 40860s 1s/step - loss: 1.4214 - accuracy: 0.4811 - val_loss: 1.4366 - val_accuracy: 0.4744\n",
      "Epoch 5/5\n",
      "36390/36390 [==============================] - 187s 5ms/step - loss: 1.4172 - accuracy: 0.4827 - val_loss: 1.4382 - val_accuracy: 0.4749\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2275/2275 [==============================] - 7s 3ms/step\n",
      "-- Result of using Deep Learning model --\n",
      "testing accuracy: 0.47\n"
     ]
    }
   ],
   "source": [
    "# Testing result\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('-- Result of using Deep Learning model --')\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert model\n",
    "Then, I try to use BERT model to do the training, since it's still the common and popular way for dealing with NLP problem these day.\n",
    " - First, I random sample 10000 data and train it, the result is roughly 45%. Also, the prediction result for the small amount type label(anger, fear, surprise) had really bad performance.As a result, I try to do the under-sampling. The label is more balanced and the dataset size also increase. The overall test performance increase, but the submission result is not that good. I found out that maybe the data distribution in the private dataset is also unbalanced, also I should let the model have more training data to learn more information.\n",
    " - Second, I try to the two stages training for the model. In the first step, I do the under-sampling to let the model learn more about all the label. In the second step, I use all the training dataset we have to train the model(Of course, it need more training time). The final testing accuracy went to 53%.\n",
    " - Third, I also try the RoBERTa model to do the emotion classification. The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach. It is a revised version of bert model. The final prediction result went to nearly 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,SequentialSampler,RandomSampler,TensorDataset,random_split\n",
    "from transformers import BertTokenizer,BertForSequenceClassification,AdamW\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'emotion'\n",
    "TEXT_COLUMN = 'preprocess_text'\n",
    "turn_labels = {\n",
    "          'sadness':0, \n",
    "          'disgust':1, \n",
    "          'anticipation':2, \n",
    "          'joy':3, \n",
    "          'trust':4, \n",
    "          'anger':5,\n",
    "          'fear':6, \n",
    "          'surprise':7\n",
    "          }\n",
    "num_labels = len(turn_labels.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum number of label\n",
    "count_dict = train_dataset.emotion.value_counts()\n",
    "min_count = min(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_8, class_7, class_6, class_5, class_4, class_3, class_2, class_1 = train_dataset.emotion.value_counts()\n",
    "c1 = train_dataset[train_dataset['emotion'] == 'joy']\n",
    "c2 = train_dataset[train_dataset['emotion'] == 'anticipation']\n",
    "c3 = train_dataset[train_dataset['emotion'] == 'trust']\n",
    "c4 = train_dataset[train_dataset['emotion'] == 'sadness']\n",
    "c5 = train_dataset[train_dataset['emotion'] == 'disgust']\n",
    "c6 = train_dataset[train_dataset['emotion'] == 'fear']\n",
    "c7 = train_dataset[train_dataset['emotion'] == 'surprise']\n",
    "c8 = train_dataset[train_dataset['emotion'] == 'anger']\n",
    "df_1 = c1.sample(min_count)\n",
    "df_2 = c2.sample(min_count)\n",
    "df_3 = c3.sample(min_count)\n",
    "df_4 = c4.sample(min_count)\n",
    "df_5 = c5.sample(min_count)\n",
    "df_6 = c6.sample(min_count)\n",
    "df_7 = c7.sample(min_count)\n",
    "df_8 = c8.sample(min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Concat all the data and shuffle it\n",
    "train_dataset = pd.concat([df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8],axis=0)\n",
    "train_dataset = shuffle(train_dataset)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_mask = []\n",
    "# Using BERT \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Using RoBERTa\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')\n",
    "\n",
    "for i in train_dataset['preprocess_text']:\n",
    "    encoded_data = tokenizer.encode_plus(\n",
    "    i,\n",
    "    add_special_tokens=True,\n",
    "    max_length=64,\n",
    "    pad_to_max_length = True,\n",
    "    truncation=True,\n",
    "    return_attention_mask= True,\n",
    "    return_tensors='pt')\n",
    "    input_ids.append(encoded_data['input_ids'])\n",
    "    attention_mask.append(encoded_data['attention_mask'])\n",
    "input_ids = torch.cat(input_ids,dim=0)\n",
    "attention_mask = torch.cat(attention_mask,dim=0)\n",
    "labels = torch.tensor([turn_labels[label] for label in train_dataset[LABEL_COLUMN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "dataset = TensorDataset(input_ids,attention_mask,labels)\n",
    "train_size = int(0.9*len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset,val_dataset = random_split(dataset,[train_size,val_size])\n",
    "\n",
    "print('Training Size - ',train_size)\n",
    "print('Validation Size - ',val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the data loader\n",
    "train_dl = DataLoader(train_dataset,sampler = RandomSampler(train_dataset),\n",
    "                     batch_size = 64)\n",
    "val_dl = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),\n",
    "                     batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds,labels):\n",
    "    pred_flat = np.argmax(preds,axis=1).flatten()\n",
    "    label_flat = labels.flatten()\n",
    "    print('----------------------------------------------')\n",
    "    print(classification_report(y_true=label_flat, y_pred=pred_flat))\n",
    "    return np.sum(pred_flat==label_flat)/len(label_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate(model, dataloader_test, device):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions,true_vals = [],[]\n",
    "    for batch in dataloader_test:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {\n",
    "            'input_ids':batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    loss_val_avg = loss_val_total / len(dataloader_test)\n",
    "    predictions = np.concatenate(predictions,axis=0)\n",
    "    true_vals = np.concatenate(true_vals,axis=0)\n",
    "    return loss_val_avg,predictions,true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from transformers import BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "SAVE_MODEL_PATH = 'dataset/trained_model/model_2'\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = 'cuda' if use_cuda else 'cpu'\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr = 2e-5, eps=1e-8)\n",
    "    total_steps = len(train_data)*epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,\n",
    "                                           num_training_steps=total_steps)\n",
    "    if use_cuda:\n",
    "        model = model.to(device)\n",
    "        print(device)\n",
    "    torch.cuda.empty_cache()       \n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        loss_train_total = 0\n",
    "        predictions,true_train = [],[]\n",
    "        for batch in tqdm(train_data):\n",
    "                \n",
    "            model.zero_grad()\n",
    "\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {\n",
    "                'input_ids':batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'labels': batch[2]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            loss_train_total += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = inputs['labels'].cpu().numpy()\n",
    "            predictions.append(logits)\n",
    "            true_train.append(label_ids)\n",
    "            if len(true_train)%1000==0:\n",
    "                model.save_pretrained(SAVE_MODEL_PATH)   \n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "                \n",
    "        model.save_pretrained(SAVE_MODEL_PATH)\n",
    "        predictions = np.concatenate(predictions,axis=0)\n",
    "        true_train = np.concatenate(true_train,axis=0)\n",
    "        train_acc = accuracy(predictions, true_train)\n",
    "        print(f'\\nEpoch {epoch_num+2}')\n",
    "        print(f'Training Accuracy: {train_acc}')\n",
    "        loss_train_avg = loss_train_total/len(train_dl)            \n",
    "        print(f'Training loss: {loss_train_avg}')\n",
    "        val_loss, predictions, true_vals = evaluate(model, val_data, device)\n",
    "        val_acc = accuracy(predictions, true_vals)\n",
    "        print(f'Validation Accuracy: {val_acc}')\n",
    "        print(f'Validation loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Bert model\n",
    "\n",
    "EPOCHS = 1\n",
    "LR = 1e-6\n",
    "PRE_MODEL_PATH = 'bert-base-uncased'\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "PRE_MODEL_PATH,\n",
    "output_attentions = False,\n",
    "output_hidden_states = False,\n",
    "num_labels = num_labels)\n",
    "\n",
    "# Using RoBERTa\n",
    "# PRE_MODEL_PATH = 'roberta-base'\n",
    "# model = RobertaForSequenceClassification.from_pretrained(\n",
    "# PRE_MODEL_PATH,\n",
    "# output_attentions = False,\n",
    "# output_hidden_states = False,\n",
    "# num_labels = num_labels)\n",
    "\n",
    "# Start training\n",
    "train(model, train_dl, val_dl, LR, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f3f3922a8c7fc5c878a4156b95aceb070294438e4791e163a27d2054f9e89d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
